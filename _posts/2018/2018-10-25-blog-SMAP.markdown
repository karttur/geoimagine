---
layout: post
title: Processing of SMAP data
modified: '2018-10-25 T18:17:25.000Z'
categories: blog
excerpt: "Processing SMAP data using Karttur's GeoImagine Framework"
image: avg-trmm-3b43v7-precip_3B43_trmm_2001-2016_A
date: '2018-10-25 T18:17:25.000Z'
comments: true
share: true
figure1: avg-grace-ave_ave-cmwater_global_2003-2016_RL05-filled
figure2A: ols-sl-grace-ave_ave-cmwater_global_2003-2016_RL05-filled
figure2B: ts-mdsl-grace-ave_ave-cmwater_global_2003-2016_RL05-filled
figure2C: ts-losl-grace-ave_ave-cmwater_global_2003-2016_RL05-filled
figure2D: ts-hisl-grace-ave_ave-cmwater_global_2003-2016_RL05-filled
---

# Introduction

This post goes through the steps needed to produce colored maps of the global soil moisture conditions as captured by the [Soil Moisture Active Passive ( SMAP)](https://smap.jpl.nasa.gov) mission. To actually repeat the steps you must have installed Karttur´s GeoImagine Framework.

<figure>
<img src="{{ site.commonurl }}/images/{{ site.data.images[page.figure1].file }}">
<figcaption> {{ site.data.images[page.figure1].caption }} </figcaption>
</figure>

# SMAP

The Soil Moisture Active Passive (SMAP) mission aimed at estimating soil moisture using a combination of active and passive microwave sensors. The active sensors only worked for a couple of months, and the original SMAP combined active and passive data are only available from 13 april 2015 to 7 july 2015.

The passive microwave instrument on SMAP original spatial resolution is 36 km, and has been operational continuously since March 2015.

More recently, algorithmic development allowed a 9 km enhanced product based on the passive sensor. Combined with the Sentinel-I active microwave sensor (C-band) a new enhanced active-passive product has also become recently available.

This blog summarises how the processing is done using Karttur´s GeoImagine Framework.

## Python Package

The GeoImagine Framework includes a package for specific SMAP processing: [geoimagine-smap](https://github.com/karttur/geoimagine-smap/). However, also several other packages in the Framework are needed for repeating the steps below.

The package includes a stand alone module <span class='module'>???</span> that can be used for adding records to the SMAP database table on templates. The templates table is required when extracting and organizing SMAP data. The templates define both which layers to extract, how to name them and where to save them.

You can accept the default templates that are installed with the Framework database, or you can redefine the dataasbe using the <span class='module'>???</span> module.

## Data access and download

The SMAP data is available via either  [Alaska Satellite Facility](https://www.asf.alaska.edu/smap/) ASF) and [National Snow and Ice Data Center](https://nsidc.org/data/smap)(NSIDC). The higher level products what are used in this post are only available from NSIDC. To get access to the data hosted by NSIDC you must sign up EART???

## Searching the SMAP online repository

The way Karttur´s GeoImagine Framework is organized, you first have to search the online repositiry and register the available SMAP data in the Framework prostgres database. Once the data is registered you can download and extract the actual SMAP data.

### Search

I have tried to find some library or database that lists the data available in the online repository, but have failed to find any. Instead I created a solution where I use wget ("web get") for downloading an html coded list of available data. The Framework process for searching the online repository for SMAP dat is <span class='package'>SearchSmapProducts</span>.

```
<?xml version='1.0' encoding='utf-8'?>
<searchsmap>
	<userproj userid = 'karttur' projectid = 'karttur' tractid= 'karttur' siteid = '*' plotid = '*' system = 'smap'></userproj>
	<period startyear = '2015' startmonth = '03' startday = '31' endyear = '2015' endmonth = '08' endday = '08' timestep='1D'></period>
	<process processid ='SearchSmapProducts' dsversion = '1.3'>
		<parameters remoteuser='YourEarthDataUser' product="SPL3SMP" version="003" serverurl="https://n5eil01u.ecs.nsidc.org/" ></parameters>
		<dstpath volume = "Karttur2tb"></dstpath>
	</process>
</searchsmap>
```

Before running the process <span class='package'>SearchSmapProducts</span> you must have the credentials for accessing [https://earthdata.nasa.gov](https://earthdata.nasa.gov) in a <span class='file'>,netrc</span> file, with the username corresponding to the one given in the xml file ('YourEarthDataUser').

The processes will the drill down into [https://earthdata.nasa.gov](https://earthdata.nasa.gov) and load all the available data as html coded files. By default the process svaes all the html files under the path <span class='file'>../smap/source/yyyy.mm.dd/</span> under the volume identified in the xml file. The files are ordinary html files, but with the <span class='file'>.html</span> extension omitted.

To actually transfer the content of the downloaded files you must run the process <span class='package'>SmapSearchToDB</span>

## Transfer search to database

To transfer the search results to the database you must run the process <span class='package'>SmapSearchToDB</span>. It reads the html files, extracts the required information and transfers it to the database. When finished it moves the html file to a sub-folder called "done". If, for some reason, you delete your database all you need to do is to take all the html files under the <span class='file'>done</span> sub-folder and move them one level up and then re-run <span class='package'>SmapSearchToDB</span>.

```
<?xml version='1.0' encoding='utf-8'?>
<searchtodb>
	<userproj userid = 'karttur' projectid = 'karttur' tractid= 'karttur' siteid = '*' plotid = '*' system = 'smap'></userproj>
	<period startyear = '2015' startmonth = '04' startday = '13' endyear = '2015' endmonth = '08' endday = '08' timestep='1D'></period>

	<process processid ='SmapSearchToDB' dsversion = '1.3'>
		<parameters product="SPL3SMAP" version="003"></parameters>
		<srcpath volume = "Karttur2tb" ></srcpath>
	</process>		
</searchtodb>
```

## Downloading SMAP data

You should now have the online available SMAP data registered in your database. You can now download any of the SMAP data in the database using the process <span class='package'>DownLoadSmapDaac</span>

I have tried to figure out how to extract individual layers from the SMAP online repository HDF5 files. But I have not managed. Hence the process <span class='package'>DownLoadSmapDaac</span> will always download the complete HDF5 files for each product and date. For most of the products this is really not a problem, but the recently available enhanced (\_E) products can be very large.

When downloading the SMAP HDF5 files you can either let the process download on the fly, or write the download command to shell script file. The latter is the default. To change it you need to set the parameter _asscript_ to False.

```
<?xml version='1.0' encoding='utf-8'?>
<downloadsmap>
	<userproj userid = 'karttur' projectid = 'karttur' tractid= 'karttur' siteid = '*' plotid = '*' system = 'smap'></userproj>
	<period startyear = '2015' startmonth = '04' startday = '13' endyear = '2015' endmonth = '08' endday = '08' timestep='1D'></period>

	<process processid ='DownLoadSmapDaac' dsversion = '1.3'>
		<parameters remoteuser='YourEarthDataUser' product="SPL3SMAP" version="003" serverurl="https://n5eil01u.ecs.nsidc.org/" ></parameters>
		<dstpath volume = "Karttur3tb" hdrfiletype = "h5" datfiletype = "h5"></dstpath>
	</process>
</downloadsmap>
```

If you did not set parameter _asscript_, including setting it to False, the process with produce a script file that you must run manually. You can also copy the script to another machine (with better internet connection) and run the script from there. THe machine you run from must have a <span class='file'>.netrc</span> file with your EarthData credentials. And the volume indicated in the xml must either exists on the machine from which you download, or you need to edit the script to reflect a volume that is available on the machine from which you download.

## Extracting SMAP layers

Tee SMAP data retrieved by Karttur´s GeoImagine Framework are as HDF5 files. The layers included in each HDF5 file, as well as metadata, can be accessed using <span class='terminalapp'><gdalinfo/span>.

<span class='terminal'>gdalinfo path/to/smap/hdf5file.H5</span>

In Karttur's GeoImagine Framework the layers to extract are defined in the database, in the table "templates" under the "smap" schema. The tempaltes table also define the celltype, cellnull, projection and the path wher to store the extracted layer.

### The EASEGRID projection

All the SMAP data layers are projected using EASEGRID 2.0, and comes in three varietes:

- A global cylidrical EASE grid (EPSG:3410)
- A northern polar EASE grid ()
- A southern polar EASE grid ()

As with all other data imported to KArttur's Geominagine Framework, the basic projection is kept, and only changed when data from different systems are combined (i.e. you can combine SMAP with MODIS by transferring the SMAP EASE GRID DATA TO MODIS sinusoidal grids).

It is best to use the EPSG code when working with the SMAP data But the EASEGRID 2.0 projection required that proj4 is at elast version 4.8.You need to check that out. Otherwise you can use the proj4 direct definition:

```
prof4
```

If you look in the module <span class='module'>hdf5_2_geotff.py</span>, you can alter between defining the projection using proj4 and the EPSG code.

The process <span class='package'>extractSmapHdf</span> extracts the layers you want from the HDF5 files.
```
<?xml version='1.0' encoding='utf-8'?>
<searchdatapool>
	<userproj userid = 'karttur' projectid = 'karttur' tractid= 'karttur' siteid = '*' plotid = '*' system = 'smap'></userproj>
	<period startyear = '2015' startmonth = '04' startday = '13' endyear = '2020' endmonth = '10' endday = '010' timestep='1D'></period>
	<process processid ='extractSmapHdf' dsversion = '1.3'>
		<parameters product="SPL3SMP" version="005" exploded ='False' remoteuser='YourEartDataUser' serverurl="https://n5eil01u.ecs.nsidc.org/"></parameters>
		<dstpath volume = "Karttur3tb"></dstpath>
	</process>
</searchdatapool>
```

The process <span class='package'>extractSmapHdf</span> will identift missing HDF5 files on the fly, and create a shell script file that, if exectured, will download the missing files. Thus you have to gice your remote unser for EARtData. If you do not have any user for EarthData and will not use the script, you can enter any name.

## Temporal resampling

The SMAP data that I use are delivered as daily timesteps. For some of the analysis and modeling that I do, the daily timestep is the best solution. But for comparing with e.g. climate data, vegetation growth, or other satellite images like MODIS, I prefer to resample the SMAP data. A lot on the processing I do either rely on weekly to biweekly, or monthly timesteps. MODIS products typically represent 8 or 16 day intervals, the old crop-climate data from AVHRR images usually come as thre (3) monthly periods, and a lot of climate data is available using monthly aggregated data.

All this can be done using Karttur's GeoImagine Framework, with the help of the <span class='package'>Pandas</package>. <span class='package'>Pandas</package> is widely used for handling timeseries data in Pyghon, and is part of Anaconda. It can be a bit slow to use for resampling, and for some of the reampling I have written numba JIT (Just In Time) compiled scripts.

### Resampling to 8 or 16 day intervals

To be completed
